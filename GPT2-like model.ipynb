{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "s4v8QaLmWXOS"
      },
      "outputs": [],
      "source": [
        "# Import of all necessary libraries\n",
        "!pip install datasets\n",
        "!pip install transformers\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import LayerNormalization, Dropout, Dense, Embedding\n",
        "from datasets import load_dataset, load_from_disk\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "from tensorflow.keras.callbacks import CSVLogger\n",
        "from google.colab import files\n",
        "import os\n",
        "from keras.saving import register_keras_serializable\n",
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.layers import GroupQueryAttention\n",
        "import datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1btZ6WzX-lMe"
      },
      "outputs": [],
      "source": [
        "# Use TensorFlow mixed precision\n",
        "from tensorflow.keras.mixed_precision import set_global_policy, Policy\n",
        "\n",
        "policy = Policy('mixed_float16')\n",
        "set_global_policy(policy)\n",
        "compute_dtype = tf.keras.mixed_precision.global_policy().compute_dtype"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ici06xaLXTrQ"
      },
      "outputs": [],
      "source": [
        "# Parameters:\n",
        "d_model = 960\n",
        "num_layers = 16\n",
        "num_heads = 16\n",
        "d_ff = 3072\n",
        "vocab_size = 30000\n",
        "max_seq_len = 128\n",
        "key_heads = 8\n",
        "dropout_rate = 0.1\n",
        "batch_size = 128\n",
        "epochs = 2\n",
        "train_dataset_path = \"/content/drive/My Drive/MyModel/train_dataset\"\n",
        "val_dataset_path = \"/content/drive/My Drive/MyModel/val_dataset\"\n",
        "drive_path = \"/content/drive/My Drive/MyModel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TXUHsCGFKPUD"
      },
      "outputs": [],
      "source": [
        "# Load a pretrained tokenizer from Drive\n",
        "tokenizer = AutoTokenizer.from_pretrained(drive_path)\n",
        "print(tokenizer.vocab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MvUbAS7mzUQs"
      },
      "outputs": [],
      "source": [
        "# Load a subset of OpenWebText and save it in Drive\n",
        "from datasets import Dataset, DatasetDict\n",
        "import random\n",
        "\n",
        "owt = load_dataset(\"openwebtext\", split=\"train\", streaming=True, trust_remote_code=True)\n",
        "dataset = owt.shuffle(buffer_size=10_000, seed=42).take(2000000)\n",
        "\n",
        "samples = []\n",
        "\n",
        "for sample in dataset:\n",
        "  samples.append(sample)\n",
        "  if len(samples) >= 2000000:\n",
        "    break\n",
        "\n",
        "samples.sort(key=lambda x: len(x[\"text\"]))\n",
        "\n",
        "samples = samples[:1000000]\n",
        "\n",
        "random.shuffle(samples)\n",
        "\n",
        "tsamples = samples[:900000]\n",
        "vsamples = samples[-100000:]\n",
        "\n",
        "train = datasets.Dataset.from_dict({\n",
        "    \"text\": [sample[\"text\"] for sample in tsamples]\n",
        "})\n",
        "\n",
        "validation = datasets.Dataset.from_dict({\n",
        "    \"text\": [sample[\"text\"] for sample in vsamples]\n",
        "})\n",
        "\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train,\n",
        "    \"validation\": validation\n",
        "})\n",
        "\n",
        "dataset_dict.save_to_disk('/content/drive/MyDrive/my_dataset')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMQIrJ5hefkn"
      },
      "outputs": [],
      "source": [
        "# Code used to train the Tokenizer\n",
        "texts = [sample[\"text\"] for sample in samples]\n",
        "\n",
        "tokenizer = Tokenizer(BPE())\n",
        "\n",
        "tokenizer.pre_tokenizer = ByteLevel()\n",
        "\n",
        "special_tokens = [\"<pad>\", \"<bos>\", \"<eos>\", \"<sep>\", \"<user>\", \"<assistant>\", \"<unk>\", \"<context>\"]\n",
        "\n",
        "trainer = BpeTrainer(\n",
        "    vocab_size=30000,\n",
        "    special_tokens=special_tokens\n",
        ")\n",
        "\n",
        "tokenizer.train_from_iterator(texts, trainer)\n",
        "\n",
        "tokenizer.save(drive_path + '/tokenizer.json')\n",
        "\n",
        "hf_tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_object=tokenizer,\n",
        "    bos_token=\"<bos>\",\n",
        "    eos_token=\"<eos>\",\n",
        "    pad_token=\"<pad>\",\n",
        "    unk_token=\"<unk>\",\n",
        ")\n",
        "\n",
        "hf_tokenizer.save_pretrained(drive_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9y6Zf8_eDml"
      },
      "outputs": [],
      "source": [
        "# Load the subset from Drive\n",
        "dataset = load_from_disk('/content/drive/MyDrive/my_dataset')\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "print(train_dataset)\n",
        "print(val_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ArXy11hZ1eKg"
      },
      "outputs": [],
      "source": [
        "# Preprocessing logic\n",
        "import re\n",
        "\n",
        "def truncate_tokens(tokens, max_len):\n",
        "    return tokens[:max_len]\n",
        "\n",
        "def split_and_truncate(text, tokenizer, max_seq_len):\n",
        "    chunks = []\n",
        "    current_chunk = []\n",
        "\n",
        "    pattern = r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.|\\?|\\!)\\s'\n",
        "    sentences = re.split(pattern, text.replace('\\n', ''))\n",
        "\n",
        "    bos_token_id = tokenizer.bos_token_id\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "    current_chunk.append(bos_token_id)\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if not sentence.strip():\n",
        "          continue\n",
        "        sentence_tokens = tokenizer.encode(sentence, add_special_tokens=False)\n",
        "        sentence_tokens = truncate_tokens(sentence_tokens, max_seq_len - 2)\n",
        "        if len(current_chunk) + len(sentence_tokens) <= max_seq_len - 1:\n",
        "            current_chunk.extend(sentence_tokens)\n",
        "        elif len(current_chunk) > 1:\n",
        "            current_chunk.append(eos_token_id)\n",
        "            chunks.append(current_chunk)\n",
        "            current_chunk = [bos_token_id] + sentence_tokens\n",
        "\n",
        "    if len(current_chunk) > 1:\n",
        "        current_chunk.append(eos_token_id)\n",
        "        chunks.append(current_chunk)\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def preprocess_function(examples, tokenizer, max_seq_len):\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    input_ids_list = []\n",
        "    labels_list = []\n",
        "\n",
        "    for text in examples[\"text\"]:\n",
        "        tokenized_chunks = split_and_truncate(text, tokenizer, max_seq_len)\n",
        "\n",
        "        for seq in tokenized_chunks:\n",
        "            input_ids = seq + [pad_token_id] * (max_seq_len - len(seq))\n",
        "\n",
        "            labels = seq[1:] + [pad_token_id] * (max_seq_len - len(seq) + 1)\n",
        "\n",
        "            input_ids_list.append(input_ids)\n",
        "            labels_list.append(labels)\n",
        "\n",
        "    return {\n",
        "        \"input_ids\": input_ids_list,\n",
        "        \"labels\": labels_list\n",
        "    }\n",
        "\n",
        "\n",
        "train_dataset = train_dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\": tokenizer, \"max_seq_len\": max_seq_len})\n",
        "val_dataset = val_dataset.map(preprocess_function, batched=True, remove_columns=[\"text\"], fn_kwargs={\"tokenizer\": tokenizer, \"max_seq_len\": max_seq_len})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T7AUf4q2Wirm"
      },
      "outputs": [],
      "source": [
        "# Saving the preprocessed dataset in Drive\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "tokenized_dataset_dict = DatasetDict({\n",
        "    \"train\": train_subset,\n",
        "    \"validation\": val_subset\n",
        "})\n",
        "\n",
        "tokenized_dataset_dict.save_to_disk('/content/drive/MyDrive/my_tokenized_dataset')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X_Ue5TS-dtna"
      },
      "outputs": [],
      "source": [
        "# Loading preprocessed dataset\n",
        "dataset = load_from_disk('/content/drive/MyDrive/my_tokenized_dataset')\n",
        "train_dataset = dataset['train']\n",
        "val_dataset = dataset['validation']\n",
        "train_dataset.remove_columns('seq_len')\n",
        "val_dataset.remove_columns('seq_len')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3rsNW0LzMiI"
      },
      "outputs": [],
      "source": [
        "# Converting the HugginFace dataset to Tensorflow dataset\n",
        "from transformers import DefaultDataCollator\n",
        "\n",
        "data_collator = DefaultDataCollator(return_tensors=\"tf\")\n",
        "\n",
        "train_tf_dataset = train_dataset.to_tf_dataset(\n",
        "    columns=\"input_ids\",\n",
        "    label_cols=\"labels\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "val_tf_dataset = val_dataset.to_tf_dataset(\n",
        "    columns=\"input_ids\",\n",
        "    label_cols=\"labels\",\n",
        "    batch_size=batch_size,\n",
        "    shuffle=False,\n",
        "    collate_fn=data_collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoEnVHvHhslz"
      },
      "outputs": [],
      "source": [
        "# Functions to create look-ahead and padding masks\n",
        "@register_keras_serializable(package='CustomTransformer', name='CreateLookAheadMask')\n",
        "def create_look_ahead_mask(seq_len, batch_size=1):\n",
        "    mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
        "    mask = tf.expand_dims(mask, axis=0)\n",
        "    mask = tf.tile(mask, [batch_size, 1, 1])\n",
        "    return tf.cast(mask, tf.bool)\n",
        "\n",
        "@register_keras_serializable(package='CustomTransformer', name='PaddingMask')\n",
        "def create_padding_mask(x):\n",
        "    padding_mask = tf.cast(tf.math.not_equal(x, 0), tf.bool)\n",
        "    padding_mask = tf.expand_dims(padding_mask, axis=1)\n",
        "    return padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7BPjGlpbhVoO"
      },
      "outputs": [],
      "source": [
        "# Creation of a custom Decoder Layer with keras\n",
        "@register_keras_serializable(package='CustomTransformer', name='DecoderLayer')\n",
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, d_model, num_heads, key_heads, d_ff, dropout_rate=0.1, **kwargs):\n",
        "        super(DecoderLayer, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.key_heads = key_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.query_head_dim = d_model // num_heads\n",
        "        self.key_value_head_dim = d_model // key_heads\n",
        "        self.gqa = GroupQueryAttention(\n",
        "            head_dim=self.query_head_dim,\n",
        "            num_query_heads=num_heads,\n",
        "            num_key_value_heads=key_heads,\n",
        "            dropout=dropout_rate,\n",
        "        )\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(d_ff, activation=\"gelu\", kernel_initializer=\"glorot_uniform\"),\n",
        "            tf.keras.layers.Dense(d_model, kernel_initializer=\"glorot_uniform\"),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "\n",
        "    def call(self, x, training=False, look_ahead_mask=None):\n",
        "        x = tf.cast(x, dtype=tf.float16)\n",
        "        x_norm = self.layernorm1(x)\n",
        "        attn_output = self.gqa(query=x_norm, key=x_norm, value=x_norm, attention_mask=look_ahead_mask, training=training)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = x + attn_output\n",
        "        ffn_output = self.ffn(self.layernorm2(out1))\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return out1 + ffn_output\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(DecoderLayer, self).get_config()\n",
        "        config.update({\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"d_ff\": self.d_ff,\n",
        "            \"key_heads\": self.key_heads,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "            \"query_head_dim\": self.query_head_dim,\n",
        "            \"key_value_head_dim\": self.key_value_head_dim\n",
        "        })\n",
        "        return config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkWO1sPuhZBh"
      },
      "outputs": [],
      "source": [
        "# Declaring a stack of Decoder Layers along with the embedding layer and positional encoding\n",
        "@register_keras_serializable(package='CustomTransformer', name='Decoder')\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_layers, d_model, num_heads, key_heads, d_ff, vocab_size, max_seq_len, dropout_rate=0.1, **kwargs):\n",
        "        super(Decoder, self).__init__(**kwargs)\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.d_ff = d_ff\n",
        "        self.key_heads = key_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = self.positional_encoding(max_seq_len, d_model)\n",
        "        self.dec_layers = [DecoderLayer(d_model, num_heads, key_heads, d_ff, dropout_rate) for _ in range(num_layers)]\n",
        "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
        "\n",
        "    def positional_encoding(self, position, d_model):\n",
        "        angle_rads = self.get_angles(\n",
        "            tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
        "            tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
        "            d_model\n",
        "        )\n",
        "        sines = tf.sin(angle_rads[:, 0::2])\n",
        "        cosines = tf.cos(angle_rads[:, 1::2])\n",
        "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
        "        return tf.cast(pos_encoding[tf.newaxis, ...], dtype=tf.float16)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_angles(pos, i, d_model):\n",
        "        angle_rates = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
        "        return tf.cast(pos, tf.float32) * angle_rates\n",
        "\n",
        "    def call(self, x, training=None, look_ahead_mask=None):\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        x = self.embedding(x) * tf.math.sqrt(tf.cast(self.d_model, tf.float16))\n",
        "        x += self.pos_encoding[:, :seq_len, :]\n",
        "        x = self.dropout(x, training=training)\n",
        "        for i in range(self.num_layers):\n",
        "            x = self.dec_layers[i](x, training=training, look_ahead_mask=look_ahead_mask)\n",
        "        return x\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Decoder, self).get_config()\n",
        "        config.update({\n",
        "            \"num_layers\": self.num_layers,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"key_heads\": self.key_heads,\n",
        "            \"d_ff\": self.d_ff,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"max_seq_len\": self.max_seq_len,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0pCCDg9Phcga"
      },
      "outputs": [],
      "source": [
        "# Declaring the decoder along with the compute of masks and the last ffn\n",
        "@register_keras_serializable(package='CustomTransformer', name='DecoderOnlyTransformer')\n",
        "class DecoderOnlyTransformer(tf.keras.models.Model):\n",
        "    def __init__(self, num_layers, d_model, num_heads, d_ff, vocab_size, max_seq_len, key_heads, dropout_rate=0.1, **kwargs):\n",
        "        super(DecoderOnlyTransformer, self).__init__(**kwargs)\n",
        "        self.decoder = Decoder(\n",
        "            num_layers=num_layers,\n",
        "            d_model=d_model,\n",
        "            num_heads=num_heads,\n",
        "            key_heads=key_heads,\n",
        "            d_ff=d_ff,\n",
        "            vocab_size=vocab_size,\n",
        "            max_seq_len=max_seq_len,\n",
        "            dropout_rate=dropout_rate)\n",
        "        self.final_layer = tf.keras.layers.Dense(vocab_size)\n",
        "        self.max_seq_len = max_seq_len\n",
        "        self.num_heads = num_heads\n",
        "        self.d_model = d_model\n",
        "        self.num_layers = num_layers\n",
        "        self.d_ff = d_ff\n",
        "        self.vocab_size = vocab_size\n",
        "        self.key_heads = key_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "    def call(self, x, training=None):\n",
        "        if isinstance(x, tuple): #Check if inputs are touples\n",
        "            x = x[0]\n",
        "        elif isinstance(x, dict): #Check if inputs are dictionaries\n",
        "            x = x[\"input_ids\"]\n",
        "        seq_len = tf.shape(x)[1]\n",
        "        batch_size = tf.shape(x)[0]\n",
        "        look_ahead_mask = create_look_ahead_mask(seq_len, batch_size)\n",
        "        padding_mask = create_padding_mask(x)\n",
        "        combined_mask = tf.logical_and(look_ahead_mask, padding_mask)\n",
        "        dec_output = self.decoder(x, training=training, look_ahead_mask=combined_mask)\n",
        "        logits = self.final_layer(dec_output)\n",
        "        return tf.cast(logits, tf.float32)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(DecoderOnlyTransformer, self).get_config()\n",
        "        config.update({\n",
        "            \"num_layers\": self.num_layers,\n",
        "            \"d_model\": self.d_model,\n",
        "            \"num_heads\": self.num_heads,\n",
        "            \"key_heads\": self.key_heads,\n",
        "            \"d_ff\": self.d_ff,\n",
        "            \"vocab_size\": self.vocab_size,\n",
        "            \"max_seq_len\": self.max_seq_len,\n",
        "            \"dropout_rate\": self.dropout_rate,\n",
        "        })\n",
        "        return config\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MfuUSErcgKn0"
      },
      "outputs": [],
      "source": [
        "# Declaration of the CallBacks\n",
        "from tensorflow.keras.callbacks import TensorBoard\n",
        "import datetime\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir logs/fit\n",
        "\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "tensorboard_callback = TensorBoard(\n",
        "    log_dir=log_dir,\n",
        "    histogram_freq=1,\n",
        "    write_graph=True,\n",
        "    write_images=True,\n",
        "    update_freq=200,\n",
        "    profile_batch=0,\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=drive_path + '/saved_model_checkpoints/modelcheckpoint.keras',\n",
        "    monitor='val_loss',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "class DebugNanCallback(tf.keras.callbacks.Callback):\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        if logs is not None and tf.math.is_nan(logs.get(\"loss\")):\n",
        "            print(f\"NaN loss encountered at batch {batch}.\")\n",
        "            self.model.stop_training = True\n",
        "debugnancallback = DebugNanCallback()\n",
        "\n",
        "callbacks = [\n",
        "    checkpoint_callback,\n",
        "    debugnancallback,\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ye-o2ezYhkxi"
      },
      "outputs": [],
      "source": [
        "# Declaration of a custom learning rate, masked loss function, masked accuracy, model and compilation of the model.\n",
        "from tensorflow.keras import losses\n",
        "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
        "\n",
        "@register_keras_serializable(package='CustomTransformer', name='warmupdecay')\n",
        "class WarmupCosineDecay(LearningRateSchedule):\n",
        "    def __init__(self, warmup_steps, initial_lr, first_decay_steps, t_mul=1.5, m_mul=1.0, alpha=0.0):\n",
        "        super().__init__()\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.initial_lr = initial_lr\n",
        "        self.first_decay_steps = first_decay_steps\n",
        "        self.t_mul = t_mul\n",
        "        self.m_mul = m_mul\n",
        "        self.alpha = alpha\n",
        "\n",
        "        self.cosine_decay = tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
        "            initial_learning_rate=initial_lr,\n",
        "            first_decay_steps=first_decay_steps,\n",
        "            t_mul=t_mul,\n",
        "            m_mul=m_mul,\n",
        "            alpha=alpha\n",
        "        )\n",
        "\n",
        "    def __call__(self, step):\n",
        "        warmup_lr = (self.initial_lr / tf.cast(self.warmup_steps, tf.float32)) * tf.cast(tf.convert_to_tensor(step), tf.float32)\n",
        "        return tf.cond(\n",
        "            step < self.warmup_steps,\n",
        "            lambda: warmup_lr,\n",
        "            lambda: self.cosine_decay(step - self.warmup_steps)\n",
        "        )\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            \"warmup_steps\": self.warmup_steps,\n",
        "            \"initial_lr\": self.initial_lr,\n",
        "            \"first_decay_steps\": self.first_decay_steps,\n",
        "            \"t_mul\": self.t_mul,\n",
        "            \"m_mul\": self.m_mul,\n",
        "            \"alpha\": self.alpha\n",
        "        }\n",
        "\n",
        "        return config\n",
        "\n",
        "@register_keras_serializable(package='CustomTransformer', name='masked_crossentropy')\n",
        "def masked_crossentropy(y_true, y_pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    loss = losses.sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)\n",
        "    mask = tf.cast(mask, dtype=loss.dtype)\n",
        "    loss *= mask\n",
        "    return tf.reduce_sum(loss) / tf.reduce_sum(mask)\n",
        "\n",
        "@register_keras_serializable(package='CustomTransformer', name='maskedaccuracy')\n",
        "def masked_accuracy(y_true, y_pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
        "    acc = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)\n",
        "    mask = tf.cast(mask, dtype=acc.dtype)\n",
        "    acc *= mask\n",
        "    return tf.reduce_sum(acc) / tf.reduce_sum(mask)\n",
        "\n",
        "model = DecoderOnlyTransformer(\n",
        "    num_layers=num_layers,\n",
        "    d_model=d_model,\n",
        "    num_heads=num_heads,\n",
        "    d_ff=d_ff,\n",
        "    key_heads=key_heads,\n",
        "    vocab_size=vocab_size,\n",
        "    max_seq_len=max_seq_len,\n",
        "    dropout_rate=dropout_rate)\n",
        "\n",
        "lr_schedule = WarmupCosineDecay(\n",
        "    warmup_steps=1000,\n",
        "    initial_lr=5e-5,\n",
        "    first_decay_steps=8000,\n",
        "    m_mul=1.0,\n",
        "    alpha=0.0\n",
        ")\n",
        "\n",
        "model.compile(\n",
        "    optimizer = tf.keras.optimizers.AdamW(\n",
        "        learning_rate=lr_schedule,\n",
        "        beta_1=0.9,\n",
        "        beta_2=0.999,\n",
        "        epsilon=1e-7,\n",
        "        weight_decay=0.01,\n",
        "        clipnorm=1.0\n",
        "    ),\n",
        "    loss=masked_crossentropy,\n",
        "    metrics=[\n",
        "        masked_accuracy\n",
        "        ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2R6DpYEfOu2B"
      },
      "outputs": [],
      "source": [
        "# Training, testing and saving logic\n",
        "import os\n",
        "\n",
        "tf.config.optimizer.set_jit(True)\n",
        "\n",
        "history = model.fit(\n",
        "    train_tf_dataset,\n",
        "    validation_data=val_tf_dataset,\n",
        "    epochs=epochs,\n",
        "    callbacks=[tensorboard_callback]\n",
        ")\n",
        "os.makedirs(drive_path, exist_ok=True)\n",
        "model_path = f'{drive_path}/astera_01.keras'\n",
        "model.save(model_path)\n",
        "val_loss, val_acc = model.evaluate(val_tf_dataset)\n",
        "print(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_acc:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "L4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}